{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Fv8AbyF15MQL"},"outputs":[],"source":["from google.colab import files\n","uploaded = files.upload()"]},{"cell_type":"markdown","metadata":{"id":"-acaBJUuFpAl"},"source":["###########         準備專家數據-要先作             ######################\n","#https://hrl.boyuai.com/chapter/3/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HsyfI_zoCqjq","executionInfo":{"status":"ok","timestamp":1703396915409,"user_tz":-480,"elapsed":459,"user":{"displayName":"Yu Alan","userId":"07954502632401534355"}}},"outputs":[],"source":["import numpy as np\n","import random\n","\n","#My Code\n","def sample_expert_dataMy():\n","    file_obs = \"MountainCar-v0_expert_states.csv\"\n","    file_act = \"MountainCar-v0_expert_actions.csv\"\n","    expert_obs = np.loadtxt(file_obs, delimiter=\"\\t\")\n","    expert_act = np.loadtxt(file_act, delimiter=\"\\t\")\n","    expert_act = expert_act.astype('int64')\n","    return expert_obs, expert_act\n","\n","expert_s, expert_a = sample_expert_dataMy()\n","\n","n_samples = 500  # 采样x个数据\n","random_index = random.sample(range(expert_s.shape[0]), n_samples)\n","expert_s = expert_s[random_index]\n","expert_a = expert_a[random_index]"]},{"cell_type":"markdown","metadata":{"id":"-UX89V6GPyN3"},"source":["############  Behavior Cloning   ###################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLLD1U-1Cqjr"},"outputs":[],"source":["import gym\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","\n","class PolicyNet(torch.nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim):\n","        super(PolicyNet, self).__init__()\n","        self.actor = nn.Sequential(\n","                        nn.Linear(state_dim, hidden_dim),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_dim, hidden_dim),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_dim, action_dim),\n","                        nn.Softmax(dim=-1)\n","                    )\n","\n","class BehaviorClone:\n","    def __init__(self, state_dim, hidden_dim, action_dim, lr):\n","        self.policy = PolicyNet(state_dim, action_dim, hidden_dim).to(device)\n","        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n","\n","    def learn(self, states, actions):\n","        states = torch.tensor(states, dtype=torch.float).to(device)\n","        actions = torch.tensor(actions).view(-1, 1).to(device)\n","        log_probs = torch.log(self.policy.actor(states).gather(1, actions))\n","        bc_loss = torch.mean(-log_probs)  # 最大似然估计; bc_loss=> 正值\n","        #bc_lossL.append(bc_loss.item())#觀察訓練誤差用\n","\n","        self.optimizer.zero_grad()\n","        bc_loss.backward()\n","        self.optimizer.step()\n","\n","    def take_action(self, state):\n","        state = torch.tensor([state], dtype=torch.float).to(device)\n","        probs = self.policy.actor(state)\n","        action_dist = torch.distributions.Categorical(probs)\n","        action = action_dist.sample()\n","        return action.item()\n","\n","\n","def test_agent(agent, env, n_episode):\n","    return_list = []\n","    for episode in range(n_episode):\n","        episode_return = 0\n","        state = env.reset()\n","        done = False\n","        while not done:\n","            action = agent.take_action(state)\n","            next_state, reward, done, _ = env.step(action)\n","            state = next_state\n","            episode_return += reward\n","        return_list.append(episode_return)\n","    return np.mean(return_list)\n","\n","\n","env_name = 'MountainCar-v0'\n","env = gym.make(env_name)\n","env.seed(0)\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","hidden_dim = 128\n","lr = 1e-3\n","bc_agent = BehaviorClone(state_dim, hidden_dim, action_dim, lr)\n","n_iterations = 5000 #訓練/測試次數ori: 1000\n","batch_size = 64\n","test_returns = []\n","#bc_lossL = []#觀察訓練誤差用\n","\n","for i in range(n_iterations):\n","  sample_indices = np.random.randint(low=0, high=expert_s.shape[0], size=batch_size)\n","  bc_agent.learn(expert_s[sample_indices], expert_a[sample_indices])\n","  current_return = test_agent(bc_agent, env, 5)#5=> 5次平均\n","  print('i: ',i,' 5次平均return: ',current_return)\n","  test_returns.append(current_return)\n","  #if current_return > -110:\n","  #  torch.save(bc_agent.policy.actor.state_dict(),'BC_'+str(i)+'.pth')\n","\n","#torch.save(bc_agent.policy.state_dict(),'bc_1000.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWdMQsYkCqjr"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","iteration_list = list(range(len(test_returns)))\n","plt.plot(iteration_list, test_returns)\n","plt.xlabel('Iterations')\n","plt.ylabel('Returns')\n","plt.title('BC on {}'.format(env_name))\n","plt.show()\n","\n","#torch.save(bc_agent.policy.state_dict(),'bcAgent.pth')#儲存模型參數"]},{"cell_type":"code","source":["iteration_list = list(range(len(bc_lossL)))\n","plt.plot(iteration_list, bc_lossL)\n","plt.xlabel('Iterations')\n","plt.ylabel('bc_loss')\n","plt.title('bc_loss on {}'.format(env_name))\n","plt.show()"],"metadata":{"id":"h7OvoZukXcxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAu808uxR6wU"},"outputs":[],"source":["#Behavior Cloning MountainCar- 測試測試測試測試測試測試測試\n","import gym\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import numpy as np\n","\n","\n","class PolicyNet(torch.nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim):\n","        super(PolicyNet, self).__init__()\n","        self.actor = nn.Sequential(\n","                        nn.Linear(state_dim, hidden_dim),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_dim, hidden_dim),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_dim, action_dim),\n","                        nn.Softmax(dim=-1)\n","                    )\n","\n","    def actT(self, stat):\n","      stat = torch.from_numpy(stat).unsqueeze(0)\n","      action = self.actor(stat)\n","      return action.argmax().item()\n","\n","env_name = 'MountainCar-v0'\n","env = gym.make(env_name)\n","num_inputs = env.observation_space.shape[0]\n","num_actions = env.action_space.n\n","hidden_dim = 128\n","\n","policy_net = PolicyNet(num_inputs, num_actions, hidden_dim)\n","#state_dict = torch.load('BC_775.pth')\n","state_dict = torch.load('/content/data/E569_PPOMC_agent.pth')\n","policy_net.actor.load_state_dict(state_dict)\n","\n","test_epochs = 100\n","rewards = []\n","for i_episode in range(test_epochs):\n","    state = env.reset()\n","    current_ep_reward = 0\n","    for t in range(300):\n","        #state = state.astype('float64')\n","        action = policy_net.actT(state)\n","        state, reward, done, _ = env.step(action)\n","\n","        current_ep_reward += reward\n","\n","        if done:\n","            print('i_episode: ',i_episode,' current_ep_reward: ',current_ep_reward)\n","            rewards.append(current_ep_reward)\n","            break\n","avg_return = np.array(rewards)\n","print('Average Return: ',np.mean(avg_return))"]},{"cell_type":"code","source":["#20231223 BC-PPO解MC: 比純PPO快很多\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.distributions import MultivariateNormal\n","from torch.distributions import Categorical\n","import random\n","import numpy as np\n","import gym\n","\n","# set device to cpu or cuda\n","device = torch.device('cpu')\n","\n","if(torch.cuda.is_available()):\n","    device = torch.device('cuda:0')\n","    torch.cuda.empty_cache()\n","    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n","else:\n","    print(\"Device set to : cpu\")\n","\n","class RolloutBuffer:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.is_terminals = []\n","\n","\n","    def clear(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.is_terminals[:]\n","\n","\n","class ActorCritic(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim):\n","        super(ActorCritic, self).__init__()\n","        self.actor = nn.Sequential(\n","                        nn.Linear(state_dim, hidden_dim),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_dim, hidden_dim),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_dim, action_dim),\n","                        nn.Softmax(dim=-1)\n","                    )\n","\n","\n","        # critic\n","        self.critic = nn.Sequential(\n","                        nn.Linear(state_dim, hidden_dim),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_dim, hidden_dim),\n","                        nn.Tanh(),\n","                        nn.Linear(hidden_dim, 1)\n","                    )\n","\n","    def act(self, state):\n","\n","        action_probs = self.actor(state)\n","        dist = Categorical(action_probs)\n","        action = dist.sample()\n","        action_logprob = dist.log_prob(action)\n","\n","        return action.detach(), action_logprob.detach()\n","\n","\n","    def evaluate(self, state, action):\n","\n","        action_probs = self.actor(state)\n","        dist = Categorical(action_probs)\n","        action_logprobs = dist.log_prob(action)\n","        dist_entropy = dist.entropy()\n","        state_values = self.critic(state)\n","\n","        return action_logprobs, state_values, dist_entropy\n","\n","class PPO:\n","    def __init__(self, state_dim, action_dim, hidden_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n","        self.gamma = gamma\n","        self.eps_clip = eps_clip\n","        self.K_epochs = K_epochs\n","\n","        self.buffer = RolloutBuffer()\n","        self.policy = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n","        state_dict = torch.load('/content/BC_697.pth')\n","        self.policy.actor.load_state_dict(state_dict)\n","        self.optimizer = torch.optim.Adam([\n","                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n","                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n","                    ])\n","\n","        self.policy_old = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","        self.MseLoss = nn.MSELoss()\n","\n","\n","    def select_action(self, state):\n","\n","        with torch.no_grad():\n","            state = torch.FloatTensor(state).to(device)\n","            action, action_logprob = self.policy_old.act(state)\n","\n","        self.buffer.states.append(state)\n","        self.buffer.actions.append(action)\n","        self.buffer.logprobs.append(action_logprob)\n","\n","        return action.item()\n","\n","\n","    def update(self):\n","\n","        # Monte Carlo estimate of returns\n","        rewards = []\n","        discounted_reward = 0\n","        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n","            if is_terminal:\n","                discounted_reward = 0\n","            discounted_reward = reward + (self.gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","\n","        # Normalizing the rewards\n","        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n","\n","        # convert list to tensor\n","        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n","        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n","        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n","\n","\n","        # Optimize policy for K epochs\n","        for _ in range(self.K_epochs):\n","\n","            # Evaluating old actions and values\n","            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n","\n","            # match state_values tensor dimensions with rewards tensor\n","            state_values = torch.squeeze(state_values)\n","\n","            # Finding the ratio (pi_theta / pi_theta__old)\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","\n","            # Finding Surrogate Loss\n","            advantages = rewards - state_values.detach()\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n","\n","            # final loss of clipped objective PPO\n","            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n","\n","            # take gradient step\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            self.optimizer.step()\n","\n","        # Copy new weights into old policy\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","        # clear buffer\n","        self.buffer.clear()\n","\n","\n","    def save(self, checkpoint_path):\n","        torch.save(self.policy_old.state_dict(), checkpoint_path)\n","\n","\n","    def load(self, checkpoint_path):\n","        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n","        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n","\n","print(\"============================================================================================\")\n","\n","def seed_torch(seed):\n","        torch.manual_seed(seed)\n","        if torch.backends.cudnn.enabled:\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = True\n","\n","####### initialize environment hyperparameters ######\n","env_name = \"MountainCar-v0\"\n","seed = 1\n","np.random.seed(seed)\n","random.seed(seed)\n","seed_torch(seed)\n","\n","################ PPO hyperparameters ################\n","K_epochs = 40               # update policy for K epochs\n","eps_clip = 0.2              # clip parameter for PPO\n","gamma = 0.99                # discount factor\n","\n","lr_actor = 0.0003       # learning rate for actor network\n","lr_critic = 0.001       # learning rate for critic network\n","#####################################################\n","\n","print(\"training environment name : \" + env_name)\n","\n","env = gym.make(env_name)\n","\n","# state space dimension\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","hidden_dim = 128 #20231222\n","#hidden_dim = 64 #20231223 純PPO時,64比128快(??)\n","################# training procedure ################\n","\n","# initialize a PPO agent\n","ppo_agent = PPO(state_dim, action_dim, hidden_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n","\n","num_epochs = 50000\n","update_epoch = 8\n","maxR = -200\n","os.makedirs('./data/', exist_ok=True)\n","for i_episode in range(num_epochs):\n","\n","    state = env.reset()\n","    current_ep_reward = 0\n","\n","    for t in range(300):\n","\n","        # select action with policy\n","        action = ppo_agent.select_action(state)\n","        state, reward, done, _ = env.step(action)\n","\n","        # saving reward and is_terminals\n","        ppo_agent.buffer.rewards.append(reward)\n","        ppo_agent.buffer.is_terminals.append(done)\n","\n","        current_ep_reward += reward\n","\n","        if done:\n","            if i_episode % update_epoch == 0:\n","                ppo_agent.update()\n","            break\n","\n","\n","\n","    if current_ep_reward > maxR:\n","      maxR = current_ep_reward\n","    if i_episode % 10 == 0:\n","      print(\"i_episode: \", i_episode, \" current_ep_reward: \",current_ep_reward,\" maxR: \",maxR)\n","\n","    if current_ep_reward > -90:\n","      fileN = './data/'+'E'+str(i_episode)+'_PPOMC_agent.pth'\n","      torch.save(ppo_agent.policy.actor.state_dict(),fileN)\n","\n","\n","#ppo_agent.policy.critic._modules['0'].weight.detach().numpy()"],"metadata":{"id":"0uvQ4at52UyH"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}